{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Nama: Wisnu Permana Putra\n",
        "\n",
        "NIM: A11.2020.12598"
      ],
      "metadata": {
        "id": "LTJVc9OqCi3X"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wDJWje_GjC1"
      },
      "source": [
        "## install library ekphrasis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVnJu5rLGll3",
        "outputId": "0a69e1a6-21ce-48fc-de1e-9f11ab9a8742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ekphrasis\n",
            "  Downloading ekphrasis-0.5.4-py3-none-any.whl (83 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/83.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m81.9/83.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.8/83.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (2.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (4.66.1)\n",
            "Collecting colorama (from ekphrasis)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting ujson (from ekphrasis)\n",
            "  Downloading ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.7.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (3.8.1)\n",
            "Collecting ftfy (from ekphrasis)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ekphrasis) (1.23.5)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->ekphrasis) (0.2.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ekphrasis) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->ekphrasis) (2023.6.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->ekphrasis) (1.16.0)\n",
            "Installing collected packages: ujson, ftfy, colorama, ekphrasis\n",
            "Successfully installed colorama-0.4.6 ekphrasis-0.5.4 ftfy-6.1.1 ujson-5.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ekphrasis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K8JCsz3GTev",
        "outputId": "f18b3a35-1fb9-4b19-8843-9a056a6cf44f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ekphrasis/classes/tokenizer.py:225: FutureWarning: Possible nested set at position 2190\n",
            "  self.tok = re.compile(r\"({})\".format(\"|\".join(pipeline)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word statistics files not found!\n",
            "Downloading... done!\n",
            "Unpacking... done!\n",
            "Reading twitter - 1grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_1grams.txt\n",
            "Reading twitter - 2grams ...\n",
            "generating cache file for faster loading...\n",
            "reading ngrams /root/.ekphrasis/stats/twitter/counts_2grams.txt\n",
            "Reading twitter - 1grams ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
            "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
          ]
        }
      ],
      "source": [
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # terms that will be normalized\n",
        "    normalize=['email', 'percent', 'money', 'phone', 'user',\n",
        "        'time', 'date', 'number'],\n",
        "    # terms that will be annotated\n",
        "    #annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",'emphasis', 'censored'},\n",
        "    annotate={\"hashtag\",\"allcaps\",\"elongated\",\"repeated\",'emphasis','censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "\n",
        "    # corpus from which the word statistics are going to be used\n",
        "    # for word segmentation\n",
        "    segmenter=\"twitter\",\n",
        "\n",
        "    # corpus from which the word statistics are going to be used\n",
        "    # for spell correction\n",
        "    corrector=\"twitter\",\n",
        "\n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=True,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "\n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=SocialTokenizer(lowercase=True).tokenize,\n",
        "\n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-RsRWIE1GZhF"
      },
      "outputs": [],
      "source": [
        "def bersih_eks(kata):\n",
        "  return \" \".join(text_processor.pre_process_doc(kata))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "43JbCIPcHOrZ"
      },
      "outputs": [],
      "source": [
        "kalimat = \"@alzami1986 berkata bahwa email alzamiCakep@gmail.com bukan emailnya, 100% yakin bukan emailnya, nomor 08564111111 juga bukan nomor hp nya maka jika ada yang meminta uang $100 atau Rp. 1.000.000 yang harus dikirim tanggal 06-10-2022 pukul 10:30 harap jangan diikuti. silakan hubungi 110. karena mereka adalah PENIPUUUUUU yang suka makan-makan 'uang tidak halal' !!! dan termasuk dalam golongan >:) #scam #penipu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "79ci5nPWISGs"
      },
      "outputs": [],
      "source": [
        "hasil01 = bersih_eks(kalimat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqafR0JaIWNt",
        "outputId": "a51aca66-a1f1-433d-b17c-675900998018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<user> berkata bahwa email <email> bukan emailnya , <percent> yakin bukan emailnya , nomor <number> juga bukan nomor hp nya maka jika ada yang meminta uang <money> atau rp . <number> . <number> yang harus dikirim tanggal <date> pukul <time> harap jangan diikuti . silakan hubungi <number> . karena mereka adalah <allcaps> penipu <elongated> </allcaps> yang suka makan - makan ' uang tidak halal ' ! <repeated> dan termasuk dalam golongan <devil> <hashtag> scam </hashtag> <hashtag> penipu </hashtag>\n"
          ]
        }
      ],
      "source": [
        "print(hasil01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhEucQfNIXQx",
        "outputId": "a2497c40-3661-4a4b-d64f-a38c35477516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "penipu <elongated>\n"
          ]
        }
      ],
      "source": [
        "kalimat2 = 'peeeeniiiiippuuuuuu'\n",
        "hasil02 = bersih_eks(kalimat2)\n",
        "print(hasil02)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y1GF3btVCgF5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}